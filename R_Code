rm(list=ls())
library(data.table)
library(Matrix)
library(xgboost)
library(caret)
library(stringr)
library(tm)
library(syuzhet) 
setwd("C:/R/Kaggle/Personalized Medicine Redefining Cancer Treatment project")

labelCountEncoding <- function(column){
  return(match(column,levels(column)[order(summary(column,maxsum=nlevels(column)))]))
}

# Load CSV files
cat("Read data")
train_text <- do.call(rbind,strsplit(readLines('C:/R/Kaggle/Personalized Medicine Redefining Cancer Treatment project/training_text.csv'),'||',fixed=T))
class(train_text)
train_text <- as.data.table(train_text)
train_text <- train_text[-1,]
colnames(train_text) <- c("ID", "Text")
train_text$ID <- as.numeric(train_text$ID)

test_text <- do.call(rbind,strsplit(readLines('C:/R/Kaggle/Personalized Medicine Redefining Cancer Treatment project/test_text.csv'),'||',fixed=T))
class(test_text)
test_text <- as.data.table(test_text)
class(test_text)
test_text <- test_text[-1,]
colnames(test_text) <- c("ID", "Text")
str(test_text)
test_text$ID <- as.numeric(test_text$ID)
str(test_text)


train <- fread("C:/R/Kaggle/Personalized Medicine Redefining Cancer Treatment project/training_variants.csv", sep=",", stringsAsFactors = T)
test <- fread("C:/R/Kaggle/Personalized Medicine Redefining Cancer Treatment project/test_variants.csv", sep=",", stringsAsFactors = T)
train <- merge(train,train_text,by="ID")
test <- merge(test,test_text,by="ID")
rm(test_text,train_text);gc()


test$Class <- -1
data <- rbind(train,test)
rm(train,test);gc()

# Basic text features
cat("Basic text features")
str(data)
data$nchar <- as.numeric(nchar(data$Text))
str(data)
data$nwords <- as.numeric(str_count(data$Text, "\\S+"))


#  Text Cleaning
cat("Text Cleaning")
txt <- Corpus(VectorSource(data$Text)) #Convert in to Corpus
txt <- tm_map(txt, stripWhitespace)
txt <- tm_map(txt, content_transformer(tolower))
txt <- tm_map(txt, removePunctuation)
txt <- tm_map(txt, removeWords, stopwords("english"))
txt <- tm_map(txt, stemDocument, language="english")
txt <- tm_map(txt, removeNumbers)

##Convert in to DTM
dtm <- DocumentTermMatrix(txt, control = list(weighting = weightTfIdf))
dtm
dtm <- removeSparseTerms(dtm, 0.95)
dtm
data <- cbind(data, as.matrix(dtm))

# LabelCount Encoding for Gene and Variation
# We can do more advanced feature engineering later, e.g. char-level n-grams
data$Gene <- labelCountEncoding(data$Gene)
data$Variation <- labelCountEncoding(data$Variation)

# Sentiment analysis
cat("Sentiment analysis")
sentiment <- get_nrc_sentiment(data$Text) 
data <- cbind(data,sentiment)


# Set seed
set.seed(2016)
cvFoldsList <- createFolds(data$Class[data$Class > -1], k=5, list=TRUE, returnTrain=FALSE)

# To sparse matrix
cat("Create sparse matrix")
varnames <- setdiff(colnames(data), c("ID", "Class", "Text"))
varnames
train_sparse <- Matrix(as.matrix(sapply(data[Class > -1, varnames, with=FALSE],as.numeric)), sparse=TRUE)
test_sparse <- Matrix(as.matrix(sapply(data[Class == -1, varnames, with=FALSE],as.numeric)), sparse=TRUE)
y_train <- data[Class > -1,Class]-1
test_ids <- data[Class == -1,ID]
dtrain <- xgb.DMatrix(data=train_sparse, label=y_train)
dtest <- xgb.DMatrix(data=test_sparse)

library(mlr)
getParamSet("classif.xgboost") #To get the list of Parameters to set
# Params for xgboost
param <- list(booster = "gbtree",
              objective = "multi:softprob",
              eval_metric = "mlogloss",
              num_class = 9,
              eta = .2,
              gamma = 1,
              max_depth = 5,
              min_child_weight = 1,
              subsample = .7,
              colsample_bytree = .7
)

# Cross validation - determine CV scores & optimal amount of rounds
cat("XGB cross validation")
xgb_cv <- xgb.cv(data = dtrain,
                 params = param,
                 nrounds = 1000,
                 maximize = FALSE,
                 prediction = TRUE,
                 folds = cvFoldsList,
                 print.every.n = 5,
                 early.stop.round = 100
)

rounds <- which.min(xgb_cv$dt[,test.mlogloss.mean])

# Train model
cat("XGB training")
xgb_model <- xgb.train(data = dtrain,
                       params = param,
                       watchlist = list(train = dtrain),
                       nrounds = 100,
                       verbose = 1,
                       print.every.n = 5)


# Feature importance
cat("Plotting feature importance")
names <- dimnames(train_sparse)[[2]]
importance_matrix <- xgb.importance(names,model=xgb_model)
xgb.plot.importance(importance_matrix[1:30,],1)


# Predict and output csv
cat("Predictions")
preds <- as.data.table(t(matrix(predict(xgb_model, dtest), nrow=9, ncol=nrow(dtest))))
colnames(preds) <- c("class1","class2","class3","class4","class5","class6","class7","class8","class9")
write.table(data.table(ID=test_ids, preds), "submission.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
